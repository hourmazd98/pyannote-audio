Neural speaker diarization with pyannote.audio

Hourmazd Delvarianzadeh 40114140111031

University: Islamic Azad University, South Tehran branch

If I want to explain my GitHub project to you in simple language, I can say that:
Instead of changing the voice,
I write the song that is in the speech. And finally, I check the note in the speech song.
Actually pyannote.audio is an open-source toolkit written in Python for speaker diarization.
Based on PyTorch machine learning framework
it provides a set of trainable end-to-end neural building blocks 
that can be combined and jointly optimized to build speaker diarization pipelines.

target:

Its purpose is to witness the process of partitioning an input audio stream into homogeneous parts in the output using the command.


Works that we did in this project : 
So far, Pyannote.audio has been updated 9 times,
but in the version 2 update, we see a complete rewrite in the program.
These changes include fundamental changes such as:
1.much better performance.Also you can see the Benchmark in the link below:


https://github.com/mahdeslami11/pyannote-audio#benchmark

pyannote.audio default speaker diarization pipeline is expected to be much better 
in v2.x than in v1.1. Those numbers are diarization error rates (in %)


2.Python-first API


3.pretrained pipelines (and models) on model hub


4.multi-GPU training with pytorch-lightning


5.data augmentation with torch-audiomentations


6.Prodigy recipes for model-assisted audio annotation

My main source code:

https://github.com/pyannote/pyannote-audio

My main account in Github:

https://github.com/hourmazd98

My Linkedin account:

https://www.linkedin.com/in/hourmazd-delvarianzadeh-321187212


